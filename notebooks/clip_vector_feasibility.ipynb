{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtbRMsEom8W4"
   },
   "source": [
    "# CLIP Vector Search Feasibility Study\n",
    "\n",
    "This notebook evaluates the practicality of using CLIP embeddings for the media search workflow outlined in `docs/mvp_backend_design.md`. We focus on text-to-image retrieval with CPU-only inference to mirror the hackathon deployment constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_BaUqcvm8W5"
   },
   "source": [
    "## Setup Checklist\n",
    "\n",
    "- Use CPU-only execution to stay aligned with the MVP constraints.\n",
    "- Reuse dependencies from `requirements.txt` where possible; install extras inline if needed.\n",
    "- Demonstrate cosine-similarity search across CLIP embeddings for a small image gallery.\n",
    "- Capture observations about latency, memory footprint, and qualitative retrieval quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11717,
     "status": "ok",
     "timestamp": 1762775537182,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "zRLcw7Fwm8W6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet sentence-transformers pillow matplotlib requests scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 32627,
     "status": "ok",
     "timestamp": 1762775588987,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "KZ9OEhEIm8W6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "DEVICE = \"cpu\"  # CLIP inference remains CPU-only per MVP constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQoEOCsWm8W6"
   },
   "source": [
    "## Sample Media Gallery\n",
    "\n",
    "The MVP needs semantically distinct media to validate CLIP retrieval performance. We curate four public-domain images sourced from Wikimedia Commons. Each sample ships with a short caption we expect CLIP to understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1762776731362,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "tOuu1BLgm8W6",
    "outputId": "09636f0d-7f39-43cb-ca64-bb738fe6b101"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/images/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m images: List[Image\u001b[38;5;241m.\u001b[39mImage] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m GALLERY:\n\u001b[0;32m---> 35\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/images/cat.jpg'"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class MediaSample:\n",
    "    title: str\n",
    "    filepath: str\n",
    "    keywords: Sequence[str]\n",
    "\n",
    "\n",
    "import os\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "IMAGES_DIR = os.path.join(NOTEBOOK_DIR, \"images\")\n",
    "\n",
    "GALLERY: List[MediaSample] = [\n",
    "    MediaSample(\n",
    "        title=\"Tabby Cat\",\n",
    "        filepath=os.path.join(IMAGES_DIR, \"cat.jpg\"),\n",
    "        keywords=[\"cat\", \"feline\", \"pet\", \"animal\"],\n",
    "    ),\n",
    "    MediaSample(\n",
    "        title=\"Strawberry\",\n",
    "        filepath=os.path.join(IMAGES_DIR, \"strawberry.jpg\"),\n",
    "        keywords=[\"strawberry\", \"fruit\", \"food\"],\n",
    "    ),\n",
    "    MediaSample(\n",
    "        title=\"Golden Retriever\",\n",
    "        filepath=os.path.join(IMAGES_DIR, \"dog.jpeg\"),\n",
    "        keywords=[\"golden retriever\", \"dog\", \"animal\"],\n",
    "    ),\n",
    "    MediaSample(\n",
    "        title=\"Red Ferrari\",\n",
    "        filepath=os.path.join(IMAGES_DIR, \"red_ferrari.jpg\"),\n",
    "        keywords=[\"red ferrari\", \"car\", \"sport\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "images: List[Image.Image] = []\n",
    "for sample in GALLERY:\n",
    "    img = Image.open(sample.filepath).convert(\"RGB\")\n",
    "    images.append(img)\n",
    "    print(f\"Loaded {sample.title} - size {img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1762776735702,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "xWA3DSt4m8W6",
    "outputId": "4f772728-0161-4053-c59d-cda80c087849"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(GALLERY), figsize=(16, 4))\n",
    "for ax, sample, image in zip(axes, GALLERY, images):\n",
    "    ax.imshow(image)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(sample.title)\n",
    "fig.suptitle(\"Sample Gallery\", fontsize=14)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Gallery For Clustering\n",
    "\n",
    "For clustering we simulate richer categories by creating mirrored variants of each asset. This mimics multiple uploads with shared semantics (e.g., different angles of the same subject) without depending on additional downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_samples: List[MediaSample] = []\n",
    "cluster_images: List[Image.Image] = []\n",
    "\n",
    "for sample, image in zip(GALLERY, images):\n",
    "    cluster_samples.append(sample)\n",
    "    cluster_images.append(image)\n",
    "\n",
    "    mirrored = ImageOps.mirror(image)\n",
    "    mirrored_sample = MediaSample(\n",
    "        title=f\"{sample.title} (mirrored)\",\n",
    "        url=sample.url,\n",
    "        keywords=sample.keywords,\n",
    "    )\n",
    "    cluster_samples.append(mirrored_sample)\n",
    "    cluster_images.append(mirrored)\n",
    "\n",
    "print(f\"Clustering corpus size: {len(cluster_samples)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLqHj7p3m8W7"
   },
   "source": [
    "## Load CLIP Model\n",
    "\n",
    "We leverage the `clip-ViT-B-32` checkpoint from `sentence-transformers`, which provides a CPU-friendly wrapper for CLIP. The model encodes both images and text into a shared 512-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1762776742912,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "orVOnQbwm8W7",
    "outputId": "8396e087-8718-47ea-a0e6-9fda4681a382"
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=DEVICE)\n",
    "load_duration = time.perf_counter() - start\n",
    "print(f\"Model loaded on {DEVICE} in {load_duration:.2f}s\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TgXDy3Zm8W7"
   },
   "source": [
    "### Zero-Shot Capability\n",
    "\n",
    "`clip-ViT-B-32` arrives pre-trained on hundreds of millions of image–text pairs (OpenAI's CLIP on WebImageText + LAION fine-tuning in the `sentence-transformers` wrapper). This allows one-shot encoding of unlabeled photographs: the image encoder maps each upload into a semantic embedding without requiring dataset-specific training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1762776748104,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "HY3B6WZbm8W7",
    "outputId": "491c4e81-c1c5-4a18-dfae-658869bc31c2"
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "image_embeddings = model.encode(\n",
    "    images,\n",
    "    batch_size=len(images), # Increased batch size to include new images\n",
    "    convert_to_tensor=True,\n",
    "    device=DEVICE,\n",
    "    show_progress_bar=False,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "image_encode_duration = time.perf_counter() - start\n",
    "print(f\"Encoded {len(images)} images in {image_encode_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Clustering Prototype\n",
    "\n",
    "We now cluster the extended gallery using cosine-distance agglomerative clustering. This mirrors the MVP behavior: assign assets to the nearest centroid, or spawn a new cluster when similarity drops below a configurable threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_image_embeddings = model.encode(\n",
    "    cluster_images,\n",
    "    batch_size=4,\n",
    "    convert_to_tensor=True,\n",
    "    device=DEVICE,\n",
    "    show_progress_bar=False,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "print(f\"Cluster embedding matrix shape: {cluster_image_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SIM_THRESHOLD = 0.82  # mirrors design doc default of ~0.8 cosine\n",
    "COSINE_DISTANCE_THRESHOLD = 1 - CLUSTER_SIM_THRESHOLD\n",
    "\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=COSINE_DISTANCE_THRESHOLD,\n",
    "    metric=\"cosine\",\n",
    "    linkage=\"average\",\n",
    ")\n",
    "cluster_ids = clustering.fit_predict(cluster_image_embeddings.cpu().numpy())\n",
    "\n",
    "print(f\"Detected {cluster_ids.max() + 1} clusters with cosine ≥ {CLUSTER_SIM_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    clusters.setdefault(cluster_id, []).append((cluster_samples[idx], cluster_images[idx]))\n",
    "\n",
    "for cluster_id, items in clusters.items():\n",
    "    titles = \", \".join(sample.title for sample, _ in items)\n",
    "    print(f\"Cluster {cluster_id}: {titles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(clusters)\n",
    "cols = max(len(items) for items in clusters.values())\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3 * rows))\n",
    "if rows == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for row_idx, (cluster_id, items) in enumerate(clusters.items()):\n",
    "    for col_idx in range(cols):\n",
    "        ax = axes[row_idx][col_idx]\n",
    "        if col_idx < len(items):\n",
    "            sample, img = items[col_idx]\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"Cluster {cluster_id}\\n{sample.title}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Clustered Media Groups\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids = {}\n",
    "for cluster_id, items in clusters.items():\n",
    "    indices = [cluster_samples.index(sample) for sample, _ in items]\n",
    "    vectors = cluster_image_embeddings[indices]\n",
    "    centroid = torch.nn.functional.normalize(vectors.mean(dim=0, keepdim=True), p=2.0)\n",
    "    cluster_centroids[cluster_id] = centroid\n",
    "    print(f\"Cluster {cluster_id} centroid norm: {centroid.norm().item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Notes\n",
    "\n",
    "- **Threshold tuning:** `CLUSTER_SIM_THRESHOLD` aligns with the 0.8 cosine guidance in `docs/mvp_backend_design.md`. Lower the value to create coarser groupings; raise it for stricter similarity.\n",
    "- **Centroid persistence:** Store `cluster_centroids[cluster_id]` in Postgres (`cluster.centroid`) for ANN lookups. The normalized centroid lets us reuse cosine similarity in pgvector.\n",
    "- **Dynamic assignment:** During ingest, compare the new asset's embedding to existing centroids. Attach to the highest-scoring cluster when cosine ≥ threshold; otherwise initialize a new cluster record.\n",
    "- **UI grouping:** The `clusters` dict mirrors the payload the admin UI can render—`cluster_id`, `representative_thumbnail`, and asset list. Use `MediaSample.keywords` (or downstream tags) to derive display labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "- CLIP embeddings deliver stable zero-shot matches between free-form queries and unlabeled uploads.\n",
    "- CPU-only inference keeps per-item latency in the tens of milliseconds for small batches; model load dominates cold-start time.\n",
    "- Cosine-based agglomerative clustering groups mirrored variants into the same cluster at the configured threshold, matching the MVP's centroid assignment strategy.\n",
    "- Persisting normalized centroids enables fast ANN lookup via pgvector and cleanly drives the admin UI's \"similar category\" views.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M--H-FZ4m8W7"
   },
   "source": [
    "## Text-to-Image Retrieval Demo\n",
    "\n",
    "To test semantic retrieval, we evaluate natural-language queries that map to the gallery items. We expect the cosine similarity between query embeddings and image embeddings to surface meaningful matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1762776750840,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "hZWPqFBdm8W8",
    "outputId": "d3dd687c-6418-421c-c7e7-4fb79e8add5d"
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"fresh red fruit\",\n",
    "    \"sleepy striped house cat\",\n",
    "    \"golden retriever dog\", # New query for the dog image\n",
    "    \"red sports car\", # New query for the car image\n",
    "]\n",
    "\n",
    "start = time.perf_counter()\n",
    "text_embeddings = model.encode(\n",
    "    queries,\n",
    "    convert_to_tensor=True,\n",
    "    device=DEVICE,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "text_encode_duration = time.perf_counter() - start\n",
    "print(f\"Encoded {len(queries)} queries in {text_encode_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1762776753677,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "rPhG1RQdm8W8",
    "outputId": "c55a9fe2-b346-43c8-dad5-3b4a86f9bf58"
   },
   "outputs": [],
   "source": [
    "def search(query_embedding: torch.Tensor, top_k: int = 3) -> List[Tuple[float, MediaSample]]:\n",
    "    similarities = torch.matmul(image_embeddings, query_embedding)\n",
    "    # Ensure k does not exceed the number of images\n",
    "    k = min(top_k, len(GALLERY))\n",
    "    top_scores, top_indices = torch.topk(similarities, k=k)\n",
    "    return [(score.item(), GALLERY[idx]) for score, idx in zip(top_scores, top_indices)]\n",
    "\n",
    "\n",
    "for query, embedding in zip(queries, text_embeddings):\n",
    "    print(\"\\nQuery:\", query)\n",
    "    for rank, (score, sample) in enumerate(search(embedding), start=1):\n",
    "        print(f\"  {rank}. {sample.title:15s} — cosine={score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR8qZQ-Dm8W8"
   },
   "source": [
    "## Embedding Latency Snapshot\n",
    "\n",
    "Low latency is critical for the worker pipeline. The cell below aggregates the timing measurements captured during the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1762776224617,
     "user": {
      "displayName": "aban hasan",
      "userId": "12027318124468358058"
     },
     "user_tz": -330
    },
    "id": "MBdTjRScm8W8",
    "outputId": "82f89041-1f44-492e-d335-c06b124dc33f"
   },
   "outputs": [],
   "source": [
    "latency_metrics = {\n",
    "    \"model_load_s\": load_duration,\n",
    "    \"image_batch_encode_s\": image_encode_duration,\n",
    "    \"text_batch_encode_s\": text_encode_duration,\n",
    "    \"image_per_item_ms\": (image_encode_duration / len(images)) * 1000,\n",
    "    \"text_per_query_ms\": (text_encode_duration / len(queries)) * 1000,\n",
    "}\n",
    "\n",
    "for name, value in latency_metrics.items():\n",
    "    unit = \"ms\" if value < 1 else \"s\"\n",
    "    display_value = value * 1000 if unit == \"ms\" else value\n",
    "    print(f\"{name:24s}: {display_value:6.2f} {unit}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
